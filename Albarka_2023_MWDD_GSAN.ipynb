{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b779a4f5-6485-42f7-a31b-b39bca9b420c",
   "metadata": {},
   "source": [
    "### I. INTRODUCTION \n",
    "\n",
    "__Context__: The Albarka program, implemented by Save the Children in Mali and funded by the US Bureau of Humanitarian Assistance (BHA), aims to encourage good nutrition, health, and hygiene practices. One of the ways these behaviors are promoted is through the GSAN (Groupes de Soutien aux Activités de Nutrition) platform.\n",
    "\n",
    "__Questions__: \n",
    "\n",
    "* What is the proportion of women participating in GSAN?\n",
    "  \n",
    "* How much has participating in GSAN impacted women's diet diversity?\n",
    "\n",
    "* How does region, as a variable,  influence the likelihood of women meeting the minimum dietary diversity ?\n",
    "  \n",
    "* To what extent does the level of education influence women's diet diversity?\n",
    "\n",
    "__Data Source__: We will use data from the Albarka 2023 annual survey to create a dataset with four key variables:\n",
    "\n",
    "* mwdd: A 0/1 (0= no, 1= yes) variable showing whether a woman consumed at least five food groups in the last 24 hours.\n",
    "\n",
    "* gsan_factor: A yes/no variable showing whether a woman participated in a GSAN in the past 12 months.\n",
    "\n",
    "* Region: The region in Mali where the woman lives.\n",
    "\n",
    "* Cercle: Cercle in Mali where the woman lives \n",
    "\n",
    "* A15:  Categorical variable for Education \n",
    "\n",
    "__Method__:\n",
    "\n",
    "* _Statistical Analysis_: We'll  generate descriptive statistic via the EDA steps and then we will test if there's a significant difference in the proportion of women with minimum diet diversity between those who participated in GSAN and those who did not.\n",
    "\n",
    "* _Supervised Machine Learning_: We will treat this as a classification problem, where the goal is to predict a category—specifically, whether a woman consumes a minimum diet (1 = yes, 0 = no) based on her participation in GSAN, region and education. To do this, we'll use logistic regression, a method commonly used for classification tasks. Logistic regression helps us predict whether something belongs to one category or another based on input data. In this case, we'll use it to predict whether a woman meets the minimum diet diversity based on her participation in GSAN, region and education.\n",
    "\n",
    "__Steps__: We will follow a six-step process. First, we'll load the necessary library and dataset to get everything ready for analysis. Next, we'll explore the data to understand its characteristics and uncover any patterns or trends, a step known as Exploratory Data Analysis (EDA). After that, we'll run a statistical test to determine if the proportions are significantly different, helping us understand any key differences in our data.\n",
    "\n",
    "In the fourth step, we'll prepare the data by encoding categorical variables, splitting the data into training and testing sets, and setting the _features_ (GSAN participation, region and education) that we'll focus on in our analysis. Once the data is prepared, we'll run a logistic regression algorithm to analyze the relationship between the features and our outcome of interest.Finally, we'll test the models to evaluate their performance and make any necessary adjustments to improve their accuracy and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58ac6f-8209-4ce7-8572-b50fcada54e7",
   "metadata": {},
   "source": [
    "### II. Libraries and dataset loading \n",
    "\n",
    "A library in programming is a collection of pre-written code that provides specific functionality, making it easier to perform common tasks without having to write the code from scratch. In Python, libraries are often packages that I can import into my project to use their features.\n",
    "\n",
    "Here are the libraries that we will be loading  and using in this project: \n",
    "\n",
    "__numpy__ : For numerical computing. It provides support for arrays, matrices, and many mathematical functions to operate on these data structures efficiently.\n",
    "\n",
    "__pandas__ : For data manipulation and analysis. It provides data structures like DataFrames, which are similar to tables in a database or Excel spreadsheet, making it easier to work with structured data.\n",
    "\n",
    "\n",
    "__statsmodel__ : This is a library that provides classes and functions for the estimation of many different statistical models. It also includes tools for statistical tests and data exploration.\n",
    "\n",
    "\n",
    "__sklearn__: This is a popular machine learning package in Python.  It offers a comprehensive libraries that covers many aspects of machine learning, from data preprocessing to model selection and evaluation, making it a go-to tool for both beginners and experts in the field. For that project, we will import 1)  _LabelEncoder_ and OneHotEncoder for converting categorical labels into numeric values that can be used in machine learning algorithms 2) _train_test_split_ fuction to split our dataset into two parts: training data and testing data 3) _LogisticRegression_ class to implement the logistic regression and 4) _metrics.classification_report_ function to evaluate the model performance after testing it by comparing the predicted labels to the actual labels in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c731d3f-bb23-4de5-88ac-40b76f7586e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6263f87c-e5dc-4f9f-97b2-f125579da81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreiving the data and printing the first five data set rows\n",
    "path = \"C:/Users/Nael/Desktop/Albarka/AS2023/data/mwdd_gsan.csv\"\n",
    "df = pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624efe42-227c-4c47-be46-dff400c250f0",
   "metadata": {},
   "source": [
    "### III. Exploratory Data Analysis (EDA) \n",
    "\n",
    "We are conducting an Exploratory Data Analysis (EDA) because it is important in machine learning as it helps us to understand our data before we start building logistic model. It’is crucial for five reasons : _1) we  get to know my data_: By exploring our dataset,  we can  see what it looks like, including the number of rows and columns, types of data (like numbers or text), and any patterns or trends.  _2) we identify issues_: we can spot problems such as missing values, outliers (unusual values), or errors. If these issues are not addressed, they can cause my model to perform poorly.  _3) we can  find Relationships_:  It  helps us discover relationships between different variables.  _4) we Make Informed Decisions_: we will make better decisions about how to prepare the data for the logistic Model. This will include how to transform variables. _5)  we avoid surprises_: If we skip EDA, we might run into unexpected issues later in the modeling process. It will help us to catch these potential problems early, saving time and effort.\n",
    "\n",
    "Through this EDA, we will ask seven questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a9cd2-056f-4150-a645-0a570a69b9d0",
   "metadata": {},
   "source": [
    "__1. Is the dataset in tabular format (row and column)?__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1bdb4f-c510-47b6-afc6-4587a70dc9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mwdd</th>\n",
       "      <th>gsan_factor</th>\n",
       "      <th>Region</th>\n",
       "      <th>A15</th>\n",
       "      <th>Cercle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>Gao</td>\n",
       "      <td>Aucun</td>\n",
       "      <td>Gao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>Gao</td>\n",
       "      <td>Ecole coranique</td>\n",
       "      <td>Ansongo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>Gao</td>\n",
       "      <td>Aucun</td>\n",
       "      <td>Gao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>Gao</td>\n",
       "      <td>Aucun</td>\n",
       "      <td>Gao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>Gao</td>\n",
       "      <td>Aucun</td>\n",
       "      <td>Gao</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mwdd gsan_factor Region              A15   Cercle\n",
       "0     0          no    Gao            Aucun      Gao\n",
       "1     0          no    Gao  Ecole coranique  Ansongo\n",
       "2     0          no    Gao            Aucun      Gao\n",
       "3     0          no    Gao            Aucun      Gao\n",
       "4     0          no    Gao            Aucun      Gao"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()     # viewing the first 5 rows of my dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08f93c-c352-442f-a2ac-861efa4569eb",
   "metadata": {},
   "source": [
    "__2. What is the size of the dataset ?__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c870d373-e0b8-4bba-b387-930b029d3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows is: 683  and number of columns is:  5\n"
     ]
    }
   ],
   "source": [
    " print(\"Number of rows is:\",df.shape[0],\" and number of columns is: \", df.shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c9226-8887-494b-a42a-9def6248c9e6",
   "metadata": {},
   "source": [
    "__3. What is the type of variable in each column and are there missing values ?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4d445d0-a329-4525-8a76-1b0a820530f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 683 entries, 0 to 682\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   mwdd         683 non-null    int64 \n",
      " 1   gsan_factor  683 non-null    object\n",
      " 2   Region       683 non-null    object\n",
      " 3   A15          683 non-null    object\n",
      " 4   Cercle       683 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 26.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a52fc5-0eaa-4d40-9d9e-8369a660d399",
   "metadata": {},
   "source": [
    "__4.What are the unique values in each column ?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e29f00c-02de-4fab-89bf-006f6340bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in mwdd: [0 1]\n",
      "Unique values in gsan_factor: ['no' 'yes']\n",
      "Unique values in Region: ['Gao' 'Tombouctou' 'Bandiagara' 'Douentza']\n",
      "Unique values in A15: ['Aucun' 'Ecole coranique' 'Primaire' 'Alphabétisé' 'Secondaire'\n",
      " 'Universitaire']\n",
      "Unique values in Cercle: ['Gao' 'Ansongo' 'Tombouctou' 'Koro' 'Douentza']\n"
     ]
    }
   ],
   "source": [
    "# Get unique values for each column\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in {column}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc4d3c5-9690-4451-8c9f-82e199de8b56",
   "metadata": {},
   "source": [
    "__5. What is the proportin of women participating in GSAN (gsan_factor class distribution) ?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e1d8ad-a40c-4337-a8a5-763d2f177b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gsan_factor\n",
       "no     0.648609\n",
       "yes    0.351391\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = df['gsan_factor'].value_counts(normalize =\"index\")\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95cafed-2278-4d66-b254-07581b21fe13",
   "metadata": {},
   "source": [
    "__6. How many women with Minimun diet diversity are participating (or not) in GSAN ?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2ddc32f-15e5-40de-83cc-c90e13d80faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsan_factor        no       yes\n",
      "mwdd                           \n",
      "0            0.764000  0.236000\n",
      "1            0.581986  0.418014\n"
     ]
    }
   ],
   "source": [
    "contingency_table = pd.crosstab(df['mwdd'],             # Create a row contingency table\n",
    "                                df['gsan_factor'],\n",
    "                                normalize  ='index') \n",
    "print(contingency_table)                                # Print the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654eeb46-1d48-49eb-a06a-ce68eae39b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mwdd                0         1\n",
      "gsan_factor                    \n",
      "no           0.431151  0.568849\n",
      "yes          0.245833  0.754167\n"
     ]
    }
   ],
   "source": [
    "contingency_table = pd.crosstab(df['gsan_factor'],      # Create a column contingency table\n",
    "                                df['mwdd'],\n",
    "                               normalize  ='index') \n",
    "print(contingency_table) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a71ad2-7c11-4264-a3ee-942af952837f",
   "metadata": {},
   "source": [
    "__7. How many women have met (or did not) the Minimum diet diversity (mwdd class distribution) ?__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45b56c28-7986-4099-b51a-be1b3dab3b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mwdd\n",
       "1    0.633968\n",
       "0    0.366032\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = df['mwdd'].value_counts(normalize =\"index\")\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5301c-9a1d-45b5-95e6-f937fe7bbbb4",
   "metadata": {},
   "source": [
    "__8. Is there a class imbalance in mwdd column ?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d498d99c-ecd4-4ada-b2ae-210d09867f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance Ratio: 1.73\n"
     ]
    }
   ],
   "source": [
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af613445-c4fb-42cf-b803-8e27523801ea",
   "metadata": {},
   "source": [
    "__CORRECTIVE ACTIONS NEEDED IN THE LOOP OF THE EDA__ \n",
    "\n",
    "* The dataset consists of 2,049 data points, organized in rows and columns, with no need for missing value treatment.\n",
    "\n",
    "* All entries are consistent, with no values falling outside the expected range.\n",
    "\n",
    "* The columns for GSAN and Region is  currently recorded as text, and it will need to be converted to numerical binary (1,0) values before applying any machine learning algorithms.\n",
    "\n",
    "* Region is also as a text with different value  into a signle column, we will also to need to convert so that  where each region will be  converted into its own binary (0,1) column. \n",
    "\n",
    "* The mwdd variable is already numeric, so no recoding is necessary. \n",
    "\n",
    " * mwdd variable  displays an imbalance ratio of 1.73, meaning that one class is 1.73 times more prevalent than the other. While this ratio indicates some degree of class imbalance, it is moderate compared to more extreme cases, such as a 9:1 or 10:1 ratio. In logistic regression, such a moderate imbalance might introduce a slight bias toward the majority class, but it is unlikely to severely affect the model's performance. Nevertheless, during model evaluation, it will be  crucial to monitor metrics like precision, recall, and F1-score for both classes. This will help ensure that the model performs well across all classes and does not underperform on the minority class, which could otherwise lead to a less accurate or biased outcome.\n",
    "\n",
    " * A15 is an object data type.  It needs to be converted to a categorical ordinal variable  such as no education is recordied as  1, basic literacy as 2, coranic school as 3,  primary school as 4, Secondary school as 5 and university as 6.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578d229c-17e4-438b-8d69-ad9e659b2036",
   "metadata": {},
   "source": [
    "### IV. Testing the difference in the proportion of women that meet the minimum diet diversity \n",
    "\n",
    "A test of significant difference in proportions is a statistical test used to determine whether the difference between the proportions of two groups is statistically significant. This kind of test is particularly useful when comparing categorical data, where the outcome is typically binary (e.g., success/failure, yes/no). We will perform the Z-Test for Proportions which is the most common test used with the following stated  Hypotheses:\n",
    "\n",
    "* \tNull Hypothesis (H₀): There is no difference in the proportions of women who consumed a minimum diet diversity between those participating in GSAN and those who did no (p1 equal p2)\n",
    "  \n",
    "*  Alternative Hypothesis (H₁): There is a difference between the proportions (p1≠p2).\n",
    "  \n",
    "We will a reject the  null hypothesis if P-Value is less than the significance level (typically 0.05),  indicating a significant difference in proportions .\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a61174e-eacc-427a-9b85-8457136cdfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-Statistic: 4.799779347351392\n",
      "P-Value: 1.5884053722939712e-06\n"
     ]
    }
   ],
   "source": [
    "# Create a column contingency table \n",
    "contingency_table = pd.crosstab(df['gsan_factor'], df['mwdd'])\n",
    "\n",
    "# Extract the counts from the table\n",
    "count = contingency_table.loc['yes', 1]  # Number of '1' in mwdd for 'yes' in gsan_factor\n",
    "nobs = contingency_table.loc['yes'].sum()  # Total number of observations for 'yes' in gsan_factor\n",
    "\n",
    "count_2 = contingency_table.loc['no', 1]  # Number of '1' in mwdd for 'no' in gsan_factor\n",
    "nobs_2 = contingency_table.loc['no'].sum()  # Total number of observations for 'no' in gsan_factor\n",
    "\n",
    "# Perform the Z-Test\n",
    "stat, p_value = sm.stats.proportions_ztest([count, count_2], [nobs, nobs_2])\n",
    "\n",
    "# Output the results\n",
    "print(f\"Z-Statistic: {stat}\")\n",
    "print(f\"P-Value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630a8e1-100b-4c40-983d-5cd753f2c3d3",
   "metadata": {},
   "source": [
    "__FINDINGS FROM THE TEST:__ \n",
    "The results we've obtained from the Z-test indicate the following:\n",
    "\n",
    "* Z-Statistic: 4.799779347351392: This value suggests that the observed difference in proportions is about 4.8 standard deviations away from the expected difference under the null hypothesis. A higher absolute Z-value indicates a more significant deviation from the null hypothesis.\n",
    "\n",
    "* P-Value: 1.5884053722939712e-06: This p-value is extremely small (approximately 0.000001588), much less than the common significance level of 0.05. This means that the null hypothesis (which states that there is no difference in proportions between the two groups) can be rejected with strong confidence.\n",
    "\n",
    "_Interpretation:_\n",
    "There is a statistically significant difference in the proportions of women with minimum diet  between the groups defined gsan_factor (participating or not pariticpating in GSAN. This means that participation in GSAN (as indicated by gsan_factor) has a significant association with whether women meet the minimum diet diversity requirement (mwdd)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b4c13-956f-42eb-95ce-c6b8ec0b782f",
   "metadata": {},
   "source": [
    "### V. Data preparation and  Features ingineering \n",
    "\n",
    "Feature engineering is the process of selecting, modifying, or creating new features (or variables) from raw data to improve the performance of machine learning models. Features are the input variables that the model uses to make predictions, and the quality and relevance of these features can significantly impact the model's accuracy and effectiveness. In our project and based on the findings of the EDA, we will limit the features engineering to : \n",
    "\n",
    "* _Feature Selection_:  selection the predictor (gsan_factor) and the depend variable (mwdd).\n",
    "   \n",
    "* _Feature Transformation_: Given the nature of the variable, there is no need to scaling, normalization, log transformation, but as the EDA showed we will need to do encoding (e.g., converting categories into numerical values) for gsan and region.  We will specifically do one-hot-encoding which consists of converts each category into a new binary column (0 or 1) for each unique category value.\n",
    "\n",
    "As we are building a logistic regression model (or  with any machine learning model), we want to ensure that the model generalizes well to new, unseen data. Therefore spliting the dataset in to training and testing set will be a crucial in preparing our data. The model will learn patterns from the training set. It will adjust its parameters to minimize errors on this data. After the model is trained, we will evaluate it on the testing set, which contains data the model hasn't seen before. This step helps us to asses how well the model is likely to perform on new, real-world data.\n",
    "Without a separate testing set, we might overestimate the model's performance because it would have \"seen\" all the data during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcee987-5451-47a4-bbc1-1ec57e6df35c",
   "metadata": {},
   "source": [
    "__Creating a new column for livelihood zone based on the cercle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d206efd2-6413-4e39-a640-2cb4c20e8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested if-then using np.where\n",
    "df['LZ_cercle'] = np.where(df['Cercle'] == \"Gao\", \"ML02_ML03_ML04\",\n",
    "                    np.where(df['Cercle'] == \"Ansongo\", \"ML02_ML03_ML04\",\n",
    "                    np.where(df['Cercle'] == \"Tombouctou\", \"ML02_ML03\",\n",
    "                    np.where(df['Cercle'] == \"Koro\", \"ML13_ML09\",\n",
    "                    np.where(df['Cercle'] == \"Douentza\", \"ML13\", \"Unknown\")))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59463bf6-0ceb-4331-b1fa-e9c77230c347",
   "metadata": {},
   "source": [
    "__Encoding gsan_factor and region column__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8628ef2-361c-4417-9dff-7a8cb8e38503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nael\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "# Fit and transform the 'gsan_factor' and 'Region' columns\n",
    "encoded_features = encoder.fit_transform(df[['Region', 'gsan_factor','LZ_cercle']])\n",
    "\n",
    "# Create DataFrame for encoded variables\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['Region', 'gsan_factor','LZ_cercle']))\n",
    "\n",
    "# Concatenate the encoded variables back to the main DataFrame\n",
    "df_encoded = pd.concat([df.drop(['Region', 'gsan_factor','LZ_cercle'], axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Display first few rows of the encoded DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585518a-a57a-4381-a5e3-92c20c0c9877",
   "metadata": {},
   "source": [
    "__Encoding A15 (education) as a categorical ordinal__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f90102d-d0f7-4d77-bf94-bb0476dd6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for ordinal categories\n",
    "category_mapping = {\n",
    "    'Aucun': 1,\n",
    "    'Alphabétisé': 2,    \n",
    "    'Ecole coranique': 3,    \n",
    "    'Primaire': 4,\n",
    "    'Secondaire': 5,\n",
    "    'Universitaire': 6\n",
    "}\n",
    "\n",
    "# Convert the column to categorical with the specified order\n",
    "df_encoded['A15'] = df['A15'].map(category_mapping).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12bccd94-3a88-422a-b004-e374769748b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mwdd</th>\n",
       "      <th>A15</th>\n",
       "      <th>Cercle</th>\n",
       "      <th>Region_Douentza</th>\n",
       "      <th>Region_Gao</th>\n",
       "      <th>Region_Tombouctou</th>\n",
       "      <th>gsan_factor_yes</th>\n",
       "      <th>LZ_cercle_ML02_ML03_ML04</th>\n",
       "      <th>LZ_cercle_ML13</th>\n",
       "      <th>LZ_cercle_ML13_ML09</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Gao</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Ansongo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Gao</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Gao</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Gao</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mwdd A15   Cercle  Region_Douentza  Region_Gao  Region_Tombouctou  \\\n",
       "0     0   1      Gao              0.0         1.0                0.0   \n",
       "1     0   3  Ansongo              0.0         1.0                0.0   \n",
       "2     0   1      Gao              0.0         1.0                0.0   \n",
       "3     0   1      Gao              0.0         1.0                0.0   \n",
       "4     0   1      Gao              0.0         1.0                0.0   \n",
       "\n",
       "   gsan_factor_yes  LZ_cercle_ML02_ML03_ML04  LZ_cercle_ML13  \\\n",
       "0              0.0                       1.0             0.0   \n",
       "1              0.0                       1.0             0.0   \n",
       "2              0.0                       1.0             0.0   \n",
       "3              0.0                       1.0             0.0   \n",
       "4              0.0                       1.0             0.0   \n",
       "\n",
       "   LZ_cercle_ML13_ML09  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  0.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294fb134-ebd9-407d-8266-ee037462603a",
   "metadata": {},
   "source": [
    "__Feature selection and split the dataset into 70% for training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa34c703-41b5-4247-b326-359b589bbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(df_encoded[[\"gsan_factor_yes\"]])  # gsan_factor as predictor \n",
    "y = np.asarray(df_encoded['mwdd'])           # mwdd as dependent \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8568788f-14a9-4b8d-b45d-2bf3a8703fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The training dataset has:  478 rows which represent 70% of the rows from the entire dataset\n"
     ]
    }
   ],
   "source": [
    "# Checking the size of the training set \n",
    "print(\" The training dataset has: \" ,X_train.shape [0], \"rows which represent 70% of the rows from the entire dataset\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4af3b7d4-e4e3-43c5-8b01-45c6bc9c8808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 165, 1: 313}\n"
     ]
    }
   ],
   "source": [
    "# Checking the  occurrences of each class of mwdd in the training set \n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d857a-f048-4049-a1c5-99ef7300c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340d615-2cbc-4c27-bf3a-dace96cbe9fa",
   "metadata": {},
   "source": [
    "### VI. Fitting the Logistic regression to our training set  with only GSAN as a predictor of mwdd\n",
    "\n",
    "Now that we have prepared the data and set the feature, we will fit the model for defining the causality and relationship for mwdd.  We therefore assume the following logistic model\n",
    "\n",
    "logit(P(mwdd=1))= β0 + β1 × gsan_factor_yes\n",
    "\n",
    "We will be fitting the logistic regression using two libraries: Statmodels  and scikit-learn. The first one will generate the model with coefficients for each predictor and the p-value while the second will generate the coefficients and is mostly use for prediction and model evaluation. There will be slight differences in the estimation of the predictors between statsmodels and scikit-learn when performing logistic regression. These differences are due in how these libraries handle certain aspects  such as optimization algorithms, convergence criteria  and regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224440a0-5936-4bd6-93a4-665369b11eb5",
   "metadata": {},
   "source": [
    "__Fitting  the logistic model using  Statsmodels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d5e95f3-6fcf-4bf8-9741-cfe51c7b6c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.627911\n",
      "         Iterations 5\n"
     ]
    }
   ],
   "source": [
    "X_train_with_const = sm.add_constant(X_train)  \n",
    "logit_model = sm.Logit(y_train, X_train_with_const)\n",
    "result = logit_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bb5687e-f399-413f-8eab-ab8d045b2282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  478\n",
      "Model:                          Logit   Df Residuals:                      476\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Wed, 04 Sep 2024   Pseudo R-squ.:                 0.02561\n",
      "Time:                        10:09:29   Log-Likelihood:                -300.14\n",
      "converged:                       True   LL-Null:                       -308.03\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.114e-05\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.3762      0.115      3.264      0.001       0.150       0.602\n",
      "x1             0.8383      0.218      3.850      0.000       0.412       1.265\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60768d-e826-462c-b940-9c0ab8efe6bf",
   "metadata": {},
   "source": [
    "__Fitting the logistic model using scikit-learn__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7b561a6-b5fc-4ad1-93d1-4e6357b2c571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the logistic regression model with training set\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68930e8a-8677-4757-abcd-eec928a13670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[0.80536063]]\n",
      "Intercept: [0.38179638]\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients and intercept\n",
    "coefficients = lr.coef_\n",
    "intercept = lr.intercept_\n",
    "\n",
    "# Print coefficients and intercept\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2694e5-55ec-4758-adcb-e83632ebb446",
   "metadata": {},
   "source": [
    "__What the fitted logistic regression reveals?__ \n",
    "\n",
    "The results from the Statsmodels analysis indicate that the predictor x1 (participation in GSAN) has a positive and significant effect (Causality) on the likelihood of the outcome y being 1 (meeting the minimum diet diversity). The very small p-value (P>|z| = 0.000) confirms that the effect of GSAN participation on meeting the minimum diet diversity is statistically significant, providing strong evidence that  participaiton in  GSAN (x1) is a meaningful predictor of meeting the minimun diet (y).\n",
    "\n",
    "The coefficient for participating in GSAN  (0.8383) represents the change in the log-odds in the minimun diet diversity (y = 1) for each one-unit increase, with all other variables held constant. When we exponentiate this coefficient, we obtain an odds ratio of 2.31, suggesting that being a member of GSAN makes a woman approximately 2.23 times more likely to meet the minimum diet diversity compared to non-members.\n",
    "\n",
    "In comparison, the Scikit-Learn analysis yielded a coefficient for participating in GSAN of 0.8053, the probability of meeting the minimum diet diversity  increases by approximately 2.71 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee7fa2-d41f-4d59-835e-f11d66788e33",
   "metadata": {},
   "source": [
    "### VII. Evaluation of the  Logistic regression with only GSAN as a predictor of mwdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3725c3a-d62a-4c46-8ebf-6ce3f5bbe078",
   "metadata": {},
   "source": [
    "To evaluate the model, we will first  make Predictions: The logitic model (lr)  will run on the test data (X_test) to predict the if the minimun diet diversity is met (y_pred).  We will then generate a classification report for comparing the predicted minimun diet diversity met (y_pred) with the actual minimun diet diversity met (y_test). The classification report will includes metrics like precision, recall, f1-score and accuracy.   Precision is the proportion of true positive predictions (correctly predicted positive instances) out of all positive predictions made by the model.Recall is the proportion of true positive predictions out of all actual positive instances. The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall. Accuracy is the proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59d91522-6756-422d-94c6-f59256a5f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nael\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Nael\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Nael\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        85\n",
      "           1       0.59      1.00      0.74       120\n",
      "\n",
      "    accuracy                           0.59       205\n",
      "   macro avg       0.29      0.50      0.37       205\n",
      "weighted avg       0.34      0.59      0.43       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict uisng  the test set\n",
    "y_pred = lr.predict(X_test)\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "# Print the report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fb4d4-6707-4244-a6b7-d43b84d3937d",
   "metadata": {},
   "source": [
    "__What evaluation report reveals?__\n",
    "\n",
    "The logistic regression model is struggling to correctly predict the negative class (class 0/ not meeting minimum diet diversity). Precision for class 0 is 0.00. This means that none of the instances predicted as class 0 were actually class 0.\n",
    "Recall for class 0is  0.00, this means that the model didn't correctly identify any of the true instances of class 0.\n",
    "F1-Score for class 0 is 0.00.  Since both precision and recall are 0, the F1-score is also 0, indicating poor performance on class 0.\n",
    "\n",
    "The report shows a significant imbalance for each class (85 for class 0/not meeting minimun diet and 120 for class 1/meeting minimun diet). This imbalance could be causing the model to favor predicting the positive class (1) over the negative class (0), leading to poor performance on class 0. This is something we said, from the EDA step, that we would pay attention to. \n",
    "\n",
    "To address this Class Imbalance , we will use the  Resampling Techniques. When dealing with imbalanced classes in a dataset, resampling techniques are used to adjust the distribution of the classes, making the dataset more balanced. This helps to improve the performance of machine learning models, which might otherwise be biased towards the majority class. Specifically, we will  use oversampling (like SMOTE) for the minority class or undersampling for the majority class to balance the dataset. Using oversampling techniques like SMOTE (Synthetic Minority Over-sampling Technique) is a great way to address class imbalance. SMOTE works by generating synthetic samples for the minority class, which helps the model learn from a more balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bd2df-8b43-4481-9bfb-cb9f49f1e33c",
   "metadata": {},
   "source": [
    "__Applying SMOTE for balanced minimun diet diversity class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05b99b30-c5d9-4108-8c65-9ffab02e618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the SMOTE function from imblearn library\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0807daac-d45d-401c-a9f7-7cbf6dd080ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying  SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49e0a808-a61e-4e37-8648-9fd51b0e710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 313, 1: 313}\n"
     ]
    }
   ],
   "source": [
    "# Checking the occurrences of each class of mwdd in the training set after after applying resampling  \n",
    "unique, counts = np.unique(y_train_res, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61720688-4330-4423-a533-9489bd2aff24",
   "metadata": {},
   "source": [
    "__Fitting the logistic model using Statsmodels on the resampled training set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbe0b74b-36c1-4514-97af-5cb6c0cc537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.675968\n",
      "         Iterations 4\n"
     ]
    }
   ],
   "source": [
    "X_train_with_const_res = sm.add_constant(X_train_res)  \n",
    "logit_model_res = sm.Logit(y_train_res, X_train_with_const_res)\n",
    "result_res = logit_model_res.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08acc3e6-3138-46f5-a5af-03c85a384202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  626\n",
      "Model:                          Logit   Df Residuals:                      624\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Wed, 04 Sep 2024   Pseudo R-squ.:                 0.02478\n",
      "Time:                        10:09:30   Log-Likelihood:                -423.16\n",
      "converged:                       True   LL-Null:                       -433.91\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.522e-06\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.2561      0.098     -2.615      0.009      -0.448      -0.064\n",
      "x1             0.8041      0.176      4.573      0.000       0.459       1.149\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(result_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd2a944-9642-4954-89e9-a7815fb9800f",
   "metadata": {},
   "source": [
    "__Fitting the logistic model using scikit-learn on the resampled training set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2122e5d7-da61-46de-b7af-e426560c3ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_res = LogisticRegression(solver='liblinear')\n",
    "lr1_res.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "933e328f-6519-457e-8c1f-2bb1a5baeab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[0.77769246]]\n",
      "Intercept: [-0.24629418]\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients and intercept for resampled data \n",
    "coefficients = lr1_res.coef_\n",
    "intercept = lr1_res.intercept_\n",
    "# Print coefficients and intercept for the resampled data\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4668675-4a16-48b6-8f3a-28763da3ef5a",
   "metadata": {},
   "source": [
    "__Evaluating logistic model fitted to the resampled training set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbf6ec9c-6317-4b71-b61c-cebb325c4f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.75      0.59        85\n",
      "           1       0.72      0.44      0.55       120\n",
      "\n",
      "    accuracy                           0.57       205\n",
      "   macro avg       0.60      0.60      0.57       205\n",
      "weighted avg       0.62      0.57      0.57       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_res = lr1_res.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae287bf-548c-4179-8325-97e8f148827d",
   "metadata": {},
   "source": [
    "__What the fitted model and evaluation report show when using the resampled dataset ?__\n",
    "\n",
    " After resampling the data, the model's performance has improved, particularly in predicting class 0. The results continue to indicate that women's participation in GSAN has a positive and significant effect on the likelihood of meeting the minimum diet diversity. The odds ratio can be calculated as exp(0.0.7776) ≈ 2.71, suggesting that being a member of GSAN makes a woman approximately 2.71 times more likely to meet the minimum diet diversity compared to non-members. With the resampled data, the intercept went from a positive value to a negative (-0.2462) with odd of 0.7817. This means that the likelihood of meeting the minimum diet diversity is about 78,17% of the likelihood of not meeting the minimun diet diversity when the women is not participating in GSAN (all predictors are set to zero). \n",
    "\n",
    "The model stands as:  logit(P(mwdd=1))= -0.24629418 +  0.77769246 gsan_factor_yes\n",
    "\n",
    "Though we have sufficient evidence for responding the questions in the project, we noticed an accuracy of 0.57, meaning the model is correctly classifying only 57% of the instances. This is not particularly high, especially if the classes are balanced, which suggests the model has room for improvement. We should consider now adding new feature in the model.  Let us add region to see how it changes the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c554df2-02df-4643-a583-845a7dfdc0cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "162965b2-47f5-4ffb-9afb-0bfc4196ac9c",
   "metadata": {},
   "source": [
    "### IX. Logistic regression with  GSAN  and region as a predictor of mwdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee2043-5196-4898-ad2d-81ee7b9c1ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed0a6608-14ac-402c-afcf-ce8b3910a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.asarray(df_encoded[[\"gsan_factor_yes\",\"Region_Douentza\",\"Region_Gao\",\"Region_Tombouctou\"]])  # gsan_factor as predictor \n",
    "y1 = np.asarray(df_encoded['mwdd'])           # mwdd as dependent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd74b150-e4a6-4af1-bd19-228abc5533f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Alternative code for Defining  feature matrix X and target vector y\n",
    "# X1 = df_encoded.drop('mwdd', axis=1)\n",
    "# y1 = df_encoded['mwdd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ef1125b-bfd0-4171-b5d3-7a0038e19500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5837c1c6-f88e-4d07-ae3d-411fe822ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res1, y_train_res1 = smote.fit_resample(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1533436b-b6bc-44d7-b223-0f517143ceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.520393\n",
      "         Iterations 7\n"
     ]
    }
   ],
   "source": [
    "X_train_with_const_res1 = sm.add_constant(X_train_res1)  \n",
    "logit_model_res1 = sm.Logit(y_train_res1, X_train_with_const_res1)\n",
    "result_res1 = logit_model_res1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ea8fdc1-af9d-43b8-9dd6-75d4f8ea0428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  626\n",
      "Model:                          Logit   Df Residuals:                      621\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Wed, 04 Sep 2024   Pseudo R-squ.:                  0.2492\n",
      "Time:                        10:09:31   Log-Likelihood:                -325.77\n",
      "converged:                       True   LL-Null:                       -433.91\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.179e-45\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.5886      0.170     -3.470      0.001      -0.921      -0.256\n",
      "x1             0.1314      0.237      0.554      0.579      -0.333       0.596\n",
      "x2             2.6981      0.287      9.416      0.000       2.136       3.260\n",
      "x3             0.2830      0.231      1.227      0.220      -0.169       0.735\n",
      "x4            -2.4717      0.611     -4.047      0.000      -3.669      -1.275\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(result_res1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37142bdd-1d3e-444b-87ab-3139afc788df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_res1 = LogisticRegression(solver='liblinear')\n",
    "lr1_res1.fit(X_train_res1, y_train_res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "948fca1b-7d8b-4e7e-aa9a-8bd71cea8b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[ 0.15320288  2.52148752  0.25779491 -1.95838839]]\n",
      "Intercept: [-0.57054151]\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients and intercept for resampled data \n",
    "coefficients = lr1_res1.coef_\n",
    "intercept = lr1_res1.intercept_\n",
    "# Print coefficients and intercept for the resampled data\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a6d25e34-dac6-49b3-b99d-0985790c4a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.98      0.72        85\n",
      "           1       0.97      0.49      0.65       120\n",
      "\n",
      "    accuracy                           0.69       205\n",
      "   macro avg       0.77      0.73      0.69       205\n",
      "weighted avg       0.81      0.69      0.68       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_res1 = lr1_res1.predict(X_test1)\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test1, y_pred_res1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd192b6f-d127-4d9a-b246-93790275301b",
   "metadata": {},
   "source": [
    "__What the results show when gsan and  region are used together as a predictor in meeting minimun diet ?__\n",
    "\n",
    "_Effect of Adding Region Variables_: The significant drop in the gsan_factor coefficient and its loss of statistical significance when region variables are added suggests that the apparent effect of GSAN membership in the first model was likely due to differences between regions rather than the effect of GSAN membership itself.\n",
    "\n",
    "_Regional Effects_: The region variables play a crucial role in explaining the likelihood of meeting the minimum diet diversity. Specifically, being in Douentza dramatically increases the odds, while being in Tombouctou decreases the odds significantly, compared to the reference region, Bandiagara."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c8be2-28fe-4c75-afc2-24c9e9a01885",
   "metadata": {},
   "source": [
    "### X. Logistic regression with  GSAN, region & Education as a predictor of mwdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7bbe6c77-ef4e-4ef4-beae-e61f26b10737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.508654\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  626\n",
      "Model:                          Logit   Df Residuals:                      620\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Wed, 04 Sep 2024   Pseudo R-squ.:                  0.2662\n",
      "Time:                        10:09:31   Log-Likelihood:                -318.42\n",
      "converged:                       True   LL-Null:                       -433.91\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.577e-48\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "0             -0.9644      0.238     -4.049      0.000      -1.431      -0.498\n",
      "1              0.1415      0.243      0.582      0.561      -0.335       0.618\n",
      "2              2.9764      0.308      9.653      0.000       2.372       3.581\n",
      "3              0.2894      0.234      1.237      0.216      -0.169       0.748\n",
      "4             -2.6354      0.628     -4.193      0.000      -3.867      -1.404\n",
      "5              0.1497      0.074      2.028      0.043       0.005       0.294\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Defining feature matrix X and target vector y\n",
    "X2 = np.asarray(df_encoded[[\"gsan_factor_yes\", \"Region_Douentza\", \"Region_Gao\", \"Region_Tombouctou\", \"A15\"]])\n",
    "y2 = np.asarray(df_encoded['mwdd']) \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize SMOTE for resampling\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data using SMOTE\n",
    "X_train_res2, y_train_res2 = smote.fit_resample(X_train2, y_train2)\n",
    "\n",
    "# Add a constant to the feature matrix for the intercept\n",
    "X_train_with_const_res2 = sm.add_constant(X_train_res2)\n",
    "\n",
    "# Convert the numpy array to a pandas DataFrame to use the replace method\n",
    "X_train_with_const_res2_df = pd.DataFrame(X_train_with_const_res2)\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "X_train_with_const_res2_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaNs from X_train_with_const_res2_df and corresponding y_train_res2 rows\n",
    "X_train_with_const_res2_df.dropna(inplace=True)\n",
    "y_train_res2 = y_train_res2[X_train_with_const_res2_df.index]\n",
    "\n",
    "# Now run the logistic regression model\n",
    "logit_model_res2 = sm.Logit(y_train_res2, X_train_with_const_res2_df)\n",
    "result_res2 = logit_model_res2.fit()\n",
    "\n",
    "# Print the summary of the logistic regression model\n",
    "print(result_res2.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf39fa-e31c-4c5d-900f-9cd43ba723e0",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a74f0d8-6645-455f-aee8-84ac4672959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.96      0.73        85\n",
      "           1       0.95      0.53      0.68       120\n",
      "\n",
      "    accuracy                           0.71       205\n",
      "   macro avg       0.77      0.74      0.70       205\n",
      "weighted avg       0.80      0.71      0.70       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add constant to test data\n",
    "X_test_with_const = sm.add_constant(X_test2)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_prob = result_res2.predict(X_test_with_const)\n",
    "\n",
    "# Convert predicted probabilities into binary predictions (threshold of 0.5)\n",
    "y_pred = [1 if x >= 0.5 else 0 for x in y_pred_prob]\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(y_test2, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f31b6-5710-4206-9445-e6c95aea5e03",
   "metadata": {},
   "source": [
    "What the results show when gsan, region and education are used together as a predictor in meeting minimun diet ?\r\n",
    "\r\n",
    "The consideration that we did in the previous model about the regional effect are still valid for this new model. We noticed that while the coefficient for GSAN membership has increased slightly it remains not statistically significant.\r\n",
    "\r\n",
    "Higher levels of education are associated with an increased likelihood of meeting the minimum diet diversity. For each one-unit increase in education level (moving from one category to the next, e.g., from no education to basic literacy), the log-odds of meeting the minimum diet diversity increase by 0.1541. Education level is therefore a significant predictor, indicating that higher education levels are associated with better dietary outcomes, independent of region and GSAN membership."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198eb09-4353-4533-ab24-08a6be1e4d3b",
   "metadata": {},
   "source": [
    "### XI. Logistic regression with GSAN, Livelihood Zone & Education as a predictor of mwdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87acd8-c1bf-4977-8d0f-9373d1a0b8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c855e785-a731-4a5e-ae52-9ee96d2330a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.513769\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  626\n",
      "Model:                          Logit   Df Residuals:                      620\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Wed, 04 Sep 2024   Pseudo R-squ.:                  0.2588\n",
      "Time:                        10:09:31   Log-Likelihood:                -321.62\n",
      "converged:                       True   LL-Null:                       -433.91\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.550e-46\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "0             -3.6606      0.660     -5.548      0.000      -4.954      -2.367\n",
      "1              0.0891      0.242      0.369      0.712      -0.385       0.563\n",
      "2              2.9805      0.630      4.732      0.000       1.746       4.215\n",
      "3              5.5773      0.681      8.190      0.000       4.243       6.912\n",
      "4              2.7604      0.633      4.361      0.000       1.520       4.001\n",
      "5              0.1488      0.074      2.018      0.044       0.004       0.293\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Defining feature matrix X and target vector y\n",
    "X3 = np.asarray(df_encoded[[\"gsan_factor_yes\", \"LZ_cercle_ML02_ML03_ML04\", \"LZ_cercle_ML13\", \"LZ_cercle_ML13_ML09\", \"A15\"]])\n",
    "y3 = np.asarray(df_encoded['mwdd']) \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize SMOTE for resampling\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data using SMOTE\n",
    "X_train_res3, y_train_res3 = smote.fit_resample(X_train3, y_train3)\n",
    "\n",
    "# Add a constant to the feature matrix for the intercept\n",
    "X_train_with_const_res3 = sm.add_constant(X_train_res3)\n",
    "\n",
    "# Convert the numpy array to a pandas DataFrame to use the replace method\n",
    "X_train_with_const_res3_df = pd.DataFrame(X_train_with_const_res3)\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "X_train_with_const_res3_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaNs from X_train_with_const_res2_df and corresponding y_train_res2 rows\n",
    "X_train_with_const_res3_df.dropna(inplace=True)\n",
    "y_train_res3 = y_train_res2[X_train_with_const_res3_df.index]\n",
    "\n",
    "# Now run the logistic regression model\n",
    "logit_model_res3 = sm.Logit(y_train_res3, X_train_with_const_res3_df)\n",
    "result_res3 = logit_model_res3.fit()\n",
    "\n",
    "# Print the summary of the logistic regression model\n",
    "print(result_res3.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bcdcb574-4749-4955-a773-8623f3021e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.96      0.73        85\n",
      "           1       0.95      0.53      0.68       120\n",
      "\n",
      "    accuracy                           0.71       205\n",
      "   macro avg       0.77      0.74      0.70       205\n",
      "weighted avg       0.80      0.71      0.70       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add constant to test data\n",
    "X_test_with_const = sm.add_constant(X_test3)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_prob = result_res3.predict(X_test_with_const)\n",
    "\n",
    "# Convert predicted probabilities into binary predictions (threshold of 0.5)\n",
    "y_pred = [1 if x >= 0.5 else 0 for x in y_pred_prob]\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(y_test3, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aedec5-b264-4e68-9a6d-3368aa8a895e",
   "metadata": {},
   "source": [
    "__What the results show when gsan,livelihood zone and education are used together as a predictor in meeting minimun diet__ ?\n",
    "\n",
    "_Effect of Adding livelihood zone Variables_: There is significant drop in the gsan_factor coefficient and a loss of statistical significance when livelihood variable are added. This drop is even greater in comparsion to when we added the region.  Again, this  suggests that the apparent effect of GSAN membership in the first model was likely due to differences between livelihood zone  rather than the effect of GSAN membership itself.\n",
    "\r\n",
    "_Livelihoodal Effec_ts: ThLivelihood zoneon variables play a crucial role in explaining the likelihood of meeting the minimum diet diversity. Specificy,y, being ibeign in ML13_ML09 (Koro), y increases the od more , compared to the referenc livelihood zone ML02_ML03 in Tombouctoura.\r\n",
    "\r\n",
    "The stark differences in the likelihood of meeting minimum diet diversity acrthe livelihoodsions indicate that local contxors, may play a crucial role in food groups availability and influencing women dietary outcomes. This suggest that the effectiveness of GSAN (Groupement de Santé et d'Action Nutritionnelle) may vary significantlylivelihoodgion. This means that a one-size-fits-all strategy for implementing GSAN might not be equally effective across differlivelihoodions. The analysis indicates that a region-livelihoodally adaptable GSAN strategy is likely necessary to maximize its impact. Tailoring the approach to the unique circumstances of elivelihood zonegion can help ensure that GSAN initiatives are more effective and equitable, ultimately improving dietary outcomes for women across different ar\n",
    "\n",
    "For education the observation in the previous modele is still valide. eas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fc345-fc2d-40d1-ab7e-962c965223ad",
   "metadata": {},
   "source": [
    "### XI. What are the next steps ?\n",
    "\n",
    "Taken alone, with no other variable in the model, we have sufficient evidence for responding the questions in the project. With the multivariate regression,  we noticed an accuracy of 0.70, meaning the model is correctly classifying only 70% of the instances. In fields of social science or marketing  where the consequences of a wrong prediction are less severe, an accuracy of around 0.70 might be considered acceptable, especially if the model provides actionable insights or is being used as part of a broader decision-making process. Including additional variables such as access to cash via UCT and income could be beneficial for your model, providing deeper insights into the factors influencing dietary diversity.  These variable will be available for the FY24 survey and these economic factors are likely to play a significant role in determining whether women can meet minimum diet diversity standards, and their inclusion could enhance the explanatory power and practical relevance of your analysis. Alternatively, model Tuning by experimenting  with different algorithms (such as decision tree) or hyperparameters, cross validation  to improve the model's performance can be also next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4904d151-b99a-440f-9bec-c3db575dee01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
